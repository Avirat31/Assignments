{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f52457-c4b2-4a9d-bd71-c2b17eafd00e",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f3852-2f6a-4845-8d3c-6603e63d7de0",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "Ans: . Simple linear regression involves finding a linear relationship between two variables: one independent variable and one dependent variable. It aims to predict the value of the dependent variable based on the value of the independent variable. For example, predicting house prices based on the area of the house. Multiple linear regression extends this concept to multiple independent variables, aiming to predict the value of the dependent variable based on multiple predictors. For example, predicting house prices based on area, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22198039-1161-43c8-b9a6-9913cad46987",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Ans:  Assumptions of linear regression include linearity, independence of errors, homoscedasticity (constant variance of errors), normality of errors, no multicollinearity (in multiple linear regression), and absence of influential outliers. These assumptions can be checked using various diagnostic plots, statistical tests, and techniques such as residual analysis, normality tests, and examining the correlation matrix for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed19ab-6004-4d08-b04a-9e832f189f4f",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "Ans: In a linear regression model, the slope represents the change in the dependent variable for a unit change in the independent variable. It indicates the direction and magnitude of the relationship between the variables. The intercept represents the value of the dependent variable when the independent variable(s) are zero. For example, in a real-world scenario of predicting a student's exam score based on the number of hours studied, the slope would represent the increase in the exam score for each additional hour studied, and the intercept would represent the expected exam score when the student hasn't studied at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc51f3-0321-44aa-8b95-8e8e57b0b800",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Ans: Gradient descent is an iterative optimization algorithm used in machine learning to minimize the cost function and find the optimal values of the model parameters. It starts with an initial set of parameter values and updates them iteratively by taking steps proportional to the negative gradient of the cost function. Gradient descent is used to train machine learning models, including linear regression, by adjusting the model parameters to minimize the difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266450e-264b-444e-b1f6-fd8776e2b478",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Ans: Multiple linear regression is an extension of simple linear regression that involves multiple independent variables to predict the value of a dependent variable. Unlike simple linear regression, which considers only one independent variable, multiple linear regression considers multiple predictors simultaneously. This allows for modeling more complex relationships and capturing the influence of multiple factors on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0c4dc-310a-401f-b922-f90f852b7898",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Ans: Multicollinearity in multiple linear regression refers to a high correlation between independent variables, which can lead to issues in interpreting the individual effects of variables and unstable coefficient estimates. It makes it difficult to determine the precise contribution of each variable to the model. Multicollinearity can be detected by calculating the correlation matrix or using statistical measures such as variance inflation factor (VIF). To address multicollinearity, one can remove correlated variables, use dimensionality reduction techniques, or incorporate domain knowledge to select the most relevant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c3f1f-da8c-4d92-9741-9dfbe4bbdae4",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Ans: Polynomial regression is a form of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. Unlike linear regression, which assumes a linear relationship, polynomial regression can capture non-linear relationships between variables. It allows for fitting curves and capturing more complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1610bd-ab77-4229-8d9d-a2ebd16e4f8d",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Ans: The advantages of polynomial regression over linear regression include the ability to model non-linear relationships, capture more complex patterns, and potentially provide better fit to the data. However, polynomial regression can also lead to overfitting, especially with higher degree polynomials, and can be more sensitive to outliers. It may require more data points to estimate the parameters accurately. Polynomial regression is preferred when there is evidence of non-linear relationships and when higher flexibility in modeling the data is needed. Linear regression is preferred when the relationship between variables is expected to be linear or when simplicity and interpretability are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4263b1-4774-4744-aba8-347cec2b29d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
