{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f982a8-0e6c-4c02-a742-c7d8fa29968f",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd5cb6-2946-46b7-9c8b-fd8950d7f4ad",
   "metadata": {},
   "source": [
    "## Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d209cd1-f635-4ffc-a32e-acacf7fc3214",
   "metadata": {},
   "source": [
    "Ans: In artificial neural networks, an activation function is a mathematical operation applied to the output of each neuron in a neural network layer. It determines whether a neuron should be activated or not, based on the weighted sum of its inputs. \n",
    "\n",
    "Activation functions introduce non-linearities into the network, allowing it to learn complex patterns in the data. Without activation functions, the neural network would simply be a linear regression model, incapable of learning more intricate relationships between the input and output data.\n",
    "\n",
    "There are various types of activation functions, each with its own characteristics and purposes. Some common activation functions include:\n",
    "\n",
    "1. **Sigmoid**: Maps the input to a range between 0 and 1. It is often used in the output layer of binary classification problems.\n",
    "\n",
    "2. **Hyperbolic Tangent (Tanh)**: Similar to the sigmoid function, but it maps the input to a range between -1 and 1. Tanh is commonly used in hidden layers of neural networks.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**: Returns the input if it is positive, otherwise returns zero. ReLU has become popular due to its simplicity and effectiveness in training deep neural networks.\n",
    "\n",
    "4. **Leaky ReLU**: Similar to ReLU, but instead of returning zero for negative inputs, it returns a small constant multiplied by the input. This helps mitigate the \"dying ReLU\" problem, where neurons can become inactive during training.\n",
    "\n",
    "5. **Softmax**: Used in the output layer for multi-class classification problems, softmax function normalizes the output into a probability distribution across multiple classes.\n",
    "\n",
    "Activation functions play a crucial role in the training and performance of neural networks, and selecting the appropriate activation function for a specific task can significantly impact the model's accuracy and convergence during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84eabf-21b8-43ff-b3fd-dd9eac251caf",
   "metadata": {},
   "source": [
    "## Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cb672-0363-44e0-b487-bd6bcdc63813",
   "metadata": {},
   "source": [
    "Ans: Common types of activation functions used in neural networks include:\n",
    "\n",
    "1. **Sigmoid Activation Function**: \n",
    "   - Formula: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - Range: (0, 1)\n",
    "   - Used in the output layer for binary classification problems.\n",
    "\n",
    "2. **Hyperbolic Tangent (Tanh) Activation Function**: \n",
    "   - Formula: \\( f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\)\n",
    "   - Range: (-1, 1)\n",
    "   - Similar to the sigmoid function, often used in hidden layers.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**: \n",
    "   - Formula: \\( f(x) = \\max(0, x) \\)\n",
    "   - Range: [0, +∞)\n",
    "   - Simple and effective, widely used in hidden layers.\n",
    "\n",
    "4. **Leaky ReLU**:\n",
    "   - Formula: \\( f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{otherwise} \\end{cases} \\)\n",
    "   - Range: (-∞, +∞)\n",
    "   - Introduces a small slope (typically α = 0.01) for negative inputs to avoid dead neurons.\n",
    "\n",
    "5. **Parametric ReLU (PReLU)**:\n",
    "   - Similar to Leaky ReLU, but α is learned during training.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU)**:\n",
    "   - Formula: \\( f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^{x} - 1), & \\text{otherwise} \\end{cases} \\)\n",
    "   - Range: (-∞, +∞)\n",
    "   - Smooth alternative to ReLU, with values for negative inputs.\n",
    "\n",
    "7. **Scaled Exponential Linear Unit (SELU)**:\n",
    "   - Similar to ELU, but has specific scaling properties that help stabilize the activations during training.\n",
    "\n",
    "8. **Softmax Activation Function**:\n",
    "   - Formula: \\( f(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{j} e^{x_{j}}} \\)\n",
    "   - Range: (0, 1)\n",
    "   - Used in the output layer for multi-class classification problems to produce probability distributions across classes.\n",
    "\n",
    "These activation functions serve different purposes and may be chosen based on the requirements of the neural network architecture and the nature of the problem being solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c6552-123b-4c7e-8486-b9bf5a0f53d0",
   "metadata": {},
   "source": [
    "## Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11531cb-07e6-4631-9923-adc2d04084d2",
   "metadata": {},
   "source": [
    "Ans: Activation functions have a profound impact on the training process and performance of a neural network in several ways:\n",
    "\n",
    "1. **Introduction of Non-Linearity**:\n",
    "   - **Why it matters**: Neural networks need to model complex relationships in data. Linear functions cannot capture these complexities, limiting the network to solving only linearly separable problems.\n",
    "   - **Effect**: Activation functions introduce non-linearities, enabling the network to learn and represent more complex patterns and functions.\n",
    "\n",
    "2. **Gradient Flow and Backpropagation**:\n",
    "   - **Why it matters**: During training, neural networks use backpropagation to update weights based on the gradient of the loss function.\n",
    "   - **Effect**: Activation functions impact the gradients' flow. For example, the sigmoid function can lead to vanishing gradients, making training deep networks difficult. ReLU and its variants help mitigate this issue by maintaining gradients during training, especially in deep networks.\n",
    "\n",
    "3. **Training Speed and Convergence**:\n",
    "   - **Why it matters**: The efficiency of the training process is crucial, especially for large-scale networks.\n",
    "   - **Effect**: Some activation functions like ReLU and its variants (Leaky ReLU, PReLU) tend to converge faster than sigmoid or tanh functions because they avoid the saturation problem (where gradients become very small and learning slows down).\n",
    "\n",
    "4. **Activation Range and Output Characteristics**:\n",
    "   - **Why it matters**: The range of activation functions affects the output distribution and the gradients.\n",
    "   - **Effect**: Functions like sigmoid and tanh squash the output into specific ranges, which can be useful for certain layers (e.g., sigmoid in the output layer for binary classification). ReLU, with its unbounded positive range, helps avoid the vanishing gradient problem and allows the network to learn more effectively.\n",
    "\n",
    "5. **Handling of Negative Inputs**:\n",
    "   - **Why it matters**: How activation functions deal with negative inputs can influence network performance.\n",
    "   - **Effect**: ReLU sets negative inputs to zero, which can lead to dead neurons (neurons that never activate). Leaky ReLU and PReLU address this by allowing a small, non-zero gradient for negative inputs, thus maintaining a small but non-zero gradient flow.\n",
    "\n",
    "6. **Output Normalization**:\n",
    "   - **Why it matters**: Ensuring that the outputs are normalized can be important, especially for classification tasks.\n",
    "   - **Effect**: Softmax is specifically designed to produce a probability distribution over multiple classes, making it ideal for the output layer in multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e7086-2b99-4dc7-85da-c530a10ef7fb",
   "metadata": {},
   "source": [
    "## Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43057719-5bc8-4559-918c-23c6fe329f22",
   "metadata": {},
   "source": [
    "Ans: The sigmoid activation function, maps any real-valued number into a range between 0 and 1. Here’s a breakdown of how it works and its pros and cons:\n",
    "\n",
    "### How the Sigmoid Activation Function Works:\n",
    "- **Input Transformation**: The function takes an input value \\( x \\) and applies the exponential function to transform it.\n",
    "- **Output Range**: The output is a value between 0 and 1, making it particularly useful for problems where a probability output is desired, such as binary classification.\n",
    "- **Gradient Calculation**: The derivative of the sigmoid function, which is necessary for backpropagation, is \\( \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) \\). This derivative is used to update the weights during the training process.\n",
    "\n",
    "### Advantages of the Sigmoid Activation Function:\n",
    "1. **Probability Interpretation**:\n",
    "   - Outputs can be interpreted as probabilities, making it useful in the output layer for binary classification tasks.\n",
    "   \n",
    "2. **Smooth Gradient**:\n",
    "   - The function is smooth and differentiable, ensuring that small changes in input result in small changes in output, which is beneficial for gradient-based optimization methods.\n",
    "\n",
    "3. **Historical Precedence**:\n",
    "   - Sigmoid was one of the first activation functions to be used and has a well-understood mathematical basis, making it easier to analyze the behavior of simple networks.\n",
    "\n",
    "### Disadvantages of the Sigmoid Activation Function:\n",
    "1. **Vanishing Gradient Problem**:\n",
    "   - For very high or very low input values, the sigmoid function's output saturates, leading to very small gradients. This can slow down or even halt the training process, particularly in deep networks, as the gradients become too small to effectively update the weights.\n",
    "\n",
    "2. **Output Not Zero-Centered**:\n",
    "   - The outputs are not centered around zero (they range between 0 and 1). This can cause issues during optimization as it can lead to slower convergence, requiring additional mechanisms like mean normalization.\n",
    "\n",
    "3. **Computational Inefficiency**:\n",
    "   - The exponential function used in the sigmoid calculation is computationally expensive, especially compared to simpler functions like ReLU, which just involve a threshold operation.\n",
    "\n",
    "4. **Limited Range**:\n",
    "   - The sigmoid function compresses input values into a narrow range, which can cause information loss, especially for larger input values. This can limit the ability of the network to learn effectively from input data with a wide dynamic range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec91f1-0a44-428b-8983-93991c7610d1",
   "metadata": {},
   "source": [
    "## Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63f68f-3c9d-42f7-8191-fa0798cba808",
   "metadata": {},
   "source": [
    "Ans: The Rectified Linear Unit (ReLU) activation function is defined as:\n",
    "\n",
    "\\[ f(x) = \\max(0, x) \\]\n",
    "\n",
    "This means that if the input \\( x \\) is positive, the output is \\( x \\); otherwise, the output is 0. \n",
    "\n",
    "### How ReLU Works:\n",
    "- **Positive Inputs**: For any input greater than zero, the function returns the input itself.\n",
    "- **Non-Positive Inputs**: For zero or negative inputs, the function returns zero.\n",
    "\n",
    "### Differences Between ReLU and Sigmoid:\n",
    "1. **Output Range**:\n",
    "   - **ReLU**: The output range is \\([0, +\\infty)\\). This means ReLU can produce any positive value or zero.\n",
    "   - **Sigmoid**: The output range is \\((0, 1)\\), compressing input values into a narrow range.\n",
    "\n",
    "2. **Nature of Non-Linearity**:\n",
    "   - **ReLU**: Introduces piecewise linearity. It is linear for positive inputs and zero for negative inputs.\n",
    "   - **Sigmoid**: Introduces a smooth, non-linear mapping of input values.\n",
    "\n",
    "3. **Gradient Characteristics**:\n",
    "   - **ReLU**: The gradient is 1 for positive inputs and 0 for negative inputs. This simplicity helps in mitigating the vanishing gradient problem but can cause dead neurons (neurons that always output zero).\n",
    "   - **Sigmoid**: The gradient is \\( \\sigma(x)(1 - \\sigma(x)) \\), which becomes very small for large positive or negative inputs, leading to the vanishing gradient problem in deep networks.\n",
    "\n",
    "4. **Computational Efficiency**:\n",
    "   - **ReLU**: Computationally efficient as it involves simple thresholding at zero.\n",
    "   - **Sigmoid**: Computationally more expensive due to the exponential function involved.\n",
    "\n",
    "5. **Biological Plausibility**:\n",
    "   - **ReLU**: Somewhat more biologically plausible as it resembles activation patterns in biological neurons, which either fire or don't.\n",
    "   - **Sigmoid**: Less biologically plausible because biological neurons typically do not have a smooth, S-shaped activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206731b-f2cb-44c6-8283-97906ad68469",
   "metadata": {},
   "source": [
    "## Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0bcea-f6cd-4f46-9570-c9361bf18abc",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) activation function offers several benefits over the sigmoid function, particularly in the context of training deep neural networks. Here are the key advantages:\n",
    "\n",
    "1. **Mitigation of the Vanishing Gradient Problem**:\n",
    "   - **ReLU**: The gradient of ReLU is either 0 or 1, ensuring that for positive inputs, the gradient remains significant and does not vanish. This property is crucial for the effective training of deep networks, where maintaining gradient magnitude is essential for backpropagation.\n",
    "   - **Sigmoid**: The gradient of the sigmoid function can become very small for large positive or negative inputs, leading to vanishing gradients. This problem makes it difficult for the network to learn effectively, especially in deeper layers.\n",
    "\n",
    "2. **Computational Efficiency**:\n",
    "   - **ReLU**: Computationally simple, involving only a thresholding operation, which makes it faster to compute. This efficiency translates to quicker training times for neural networks.\n",
    "   - **Sigmoid**: Involves computing the exponential function, which is more computationally expensive and slows down the training process.\n",
    "\n",
    "3. **Sparse Activation**:\n",
    "   - **ReLU**: Produces sparse activations, meaning many neurons output zero for a given input. This sparsity can lead to more efficient computations and can help prevent overfitting by reducing interdependencies between neurons.\n",
    "   - **Sigmoid**: Activations are less sparse, as the sigmoid function outputs values in a continuous range (0, 1), leading to denser activation patterns.\n",
    "\n",
    "4. **Better Gradient Propagation**:\n",
    "   - **ReLU**: Helps propagate gradients more effectively, particularly in deep networks. This effective gradient propagation contributes to faster convergence during training.\n",
    "   - **Sigmoid**: Often leads to slower convergence due to the vanishing gradient issue, particularly in deep networks.\n",
    "\n",
    "5. **Non-Saturating Activation**:\n",
    "   - **ReLU**: Does not saturate in the positive region, which means that it can help avoid issues related to gradient saturation and maintain a consistent gradient during training.\n",
    "   - **Sigmoid**: Can saturate in both the positive and negative extremes, causing gradients to become very small and hindering learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735d600-b40f-4bce-aafd-44accc83db27",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d079fc-07da-417c-9a08-4887f6e3fe12",
   "metadata": {},
   "source": [
    "Ans: Leaky ReLU is a variant of the Rectified Linear Unit (ReLU) activation function that addresses the issue of \"dying ReLU\" or the vanishing gradient problem encountered in deep neural networks. It introduces a small slope for negative inputs, allowing the activation function to have a non-zero output even for negative values. Here's how it works and how it helps alleviate the vanishing gradient problem:\n",
    "\n",
    "Leaky ReLU Activation Function:\n",
    "The Leaky ReLU function is defined as follows:\n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "{\n",
    "𝑥\n",
    ",\n",
    "if \n",
    "𝑥\n",
    ">\n",
    "0\n",
    "𝛼\n",
    "𝑥\n",
    ",\n",
    "otherwise\n",
    "f(x)={ \n",
    "x,\n",
    "αx,\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "Where \n",
    "𝛼\n",
    "α is a small positive constant (typically around 0.01) called the leak coefficient. When \n",
    "𝑥\n",
    ">\n",
    "0\n",
    "x>0, it behaves like a standard ReLU, returning the input value. However, for \n",
    "𝑥\n",
    "≤\n",
    "0\n",
    "x≤0, it introduces a small linear slope, allowing a small, non-zero gradient to flow through the neuron.\n",
    "\n",
    "Addressing the Vanishing Gradient Problem:\n",
    "The vanishing gradient problem occurs when gradients in deep networks become very small, causing the network to learn slowly or not at all. Leaky ReLU helps address this problem in the following ways:\n",
    "\n",
    "#### 1. Non-Zero Gradient for Negative Inputs:\n",
    "\n",
    "  - Unlike traditional ReLU, which outputs zero for negative inputs, Leaky ReLU returns a small, non-zero value proportional to the input for negative values. This ensures that gradients do not completely vanish, allowing for continued learning even for negative inputs. \n",
    "  \n",
    "#### 2. Prevention of \"Dying\" Neurons:\n",
    "\n",
    "  - In standard ReLU, if a neuron's output is consistently zero for all inputs during training, it effectively becomes \"dead\" and stops learning. By introducing a small slope for negative inputs, Leaky ReLU prevents neurons from dying out, ensuring that they can still contribute to the network's learning process.\n",
    "\n",
    "#### 3. Improved Gradient Flow:\n",
    "\n",
    "  - The non-zero gradient for negative inputs helps maintain a more consistent gradient flow during backpropagation, especially in deeper layers of the network. This helps mitigate the vanishing gradient problem and facilitates more stable and efficient training.\n",
    "\n",
    "### Benefits of Leaky ReLU:\n",
    "#### 1. Effective in Deep Networks:\n",
    "\n",
    "  - Leaky ReLU is particularly effective in deep neural networks where the vanishing gradient problem is more pronounced. Its ability to maintain non-zero gradients for negative inputs helps ensure smoother and more consistent training.\n",
    "\n",
    "#### 2. Prevents Dead Neurons:\n",
    "\n",
    "  - By allowing a small amount of information to flow through even for negative inputs, Leaky ReLU helps prevent neurons from becoming inactive and ensures that all neurons contribute meaningfully to the network's learning process.\n",
    "#### 3. Simple Implementation:\n",
    "\n",
    "  - Leaky ReLU is straightforward to implement and computationally efficient, making it a practical choice for various deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1881ec4-1aa8-4b7d-a8bb-a8e4b941a06d",
   "metadata": {},
   "source": [
    "## Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f98cd3-4777-4041-bfc9-5a05ff8c2fa2",
   "metadata": {},
   "source": [
    "The softmax activation function is used to convert a vector of raw scores (also called logits) into probabilities. It is particularly useful in the context of multi-class classification problems where an instance can belong to one of many classes. The function ensures that the output values are in the range (0, 1) and that they sum up to 1, making them interpretable as probabilities.\n",
    "\n",
    "### Purpose of the Softmax Activation Function:\n",
    "1. **Probability Distribution**:\n",
    "   - The primary purpose of the softmax function is to transform the raw scores from the final layer of a neural network into a probability distribution. This allows each output to represent the probability that a given input belongs to each class.\n",
    "\n",
    "2. **Interpretable Outputs**:\n",
    "   - By converting logits into probabilities, the softmax function makes the network's predictions interpretable. This is crucial in classification tasks where understanding the likelihood of each class is important.\n",
    "\n",
    "3. **Normalization**:\n",
    "   - The softmax function normalizes the logits, ensuring that the total sum of the probabilities equals 1. This property is essential for tasks that require a mutually exclusive class prediction.\n",
    "\n",
    "### Mathematical Definition:\n",
    "Given a vector of logits \\( z = [z_1, z_2, \\ldots, z_K] \\), the softmax function computes the probability of the \\( i \\)-th class as follows:\n",
    "\\[ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\]\n",
    "Where \\( K \\) is the number of classes.\n",
    "\n",
    "### Common Use Cases:\n",
    "1. **Multi-Class Classification**:\n",
    "   - **Output Layer**: Softmax is commonly used in the output layer of neural networks designed for multi-class classification problems. Here, the network predicts the probability distribution over \\( K \\) different classes.\n",
    "   - **Examples**: Image classification (e.g., recognizing different objects in an image), text classification (e.g., sentiment analysis), and other tasks where an instance can belong to one of several classes.\n",
    "\n",
    "2. **Training with Cross-Entropy Loss**:\n",
    "   - **Loss Function**: Softmax is often used in conjunction with the cross-entropy loss function. The cross-entropy loss measures the difference between the predicted probability distribution (from softmax) and the true distribution (one-hot encoded vector), making it suitable for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438676a9-80de-4128-9376-baa4076adb13",
   "metadata": {},
   "source": [
    "## Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e3724-146f-4a84-a739-78a1cc0949a1",
   "metadata": {},
   "source": [
    "Ans: The hyperbolic tangent (tanh) activation function is another commonly used activation function in neural networks.\n",
    "This function maps input values to an output range between -1 and 1.\n",
    "\n",
    "### Characteristics of the tanh Function:\n",
    "- **Range**: The output values range from -1 to 1.\n",
    "- **Symmetry**: The tanh function is zero-centered, meaning its output is symmetric around the origin. This can be beneficial for training neural networks as it helps to ensure that the activations are more balanced around zero.\n",
    "- **Gradient**: The derivative of tanh is \\( 1 - \\text{tanh}^2(x) \\). This means the gradient is highest around the origin and decreases towards zero as the input moves away from zero, but it still retains a wider range of effective gradients compared to the sigmoid function.\n",
    "\n",
    "### Comparison with the Sigmoid Function:\n",
    "Both the tanh and sigmoid functions are S-shaped (sigmoid) curves, but they have some important differences:\n",
    "\n",
    "1. **Output Range**:\n",
    "   - **Tanh**: Outputs range from -1 to 1.\n",
    "   - **Sigmoid**: Outputs range from 0 to 1.\n",
    "\n",
    "2. **Zero-Centered Outputs**:\n",
    "   - **Tanh**: The output is zero-centered, which can help with balancing the activations in the network and can make the gradient updates more effective.\n",
    "   - **Sigmoid**: The output is not zero-centered, which can sometimes lead to issues during gradient descent as the gradients can tend to push updates in specific directions, potentially leading to less efficient learning.\n",
    "\n",
    "3. **Gradient Saturation**:\n",
    "   - **Tanh**: While tanh also saturates for large positive or negative inputs, the saturation effect is less severe compared to the sigmoid function. This means the gradients do not diminish as quickly as with the sigmoid function.\n",
    "   - **Sigmoid**: The sigmoid function can cause the vanishing gradient problem more severely, as the gradients become very small for inputs far from zero, making it difficult for the network to learn.\n",
    "\n",
    "4. **Use Cases**:\n",
    "   - **Tanh**: Often preferred in hidden layers of neural networks because of its zero-centered output and generally better gradient properties.\n",
    "   - **Sigmoid**: Commonly used in the output layer for binary classification problems due to its probabilistic interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a14c4-5fb9-468b-920f-c765173f0606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
