{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb5b74c-a92a-4860-a479-cbacbe9c39a0",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b9169-c34b-46a6-b890-a8420db91a42",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans: Gradient Boosting Regression is a machine learning technique that uses an ensemble of decision tree regressors to make predictions. It builds trees sequentially, with each new tree correcting the errors made by the previous ones, optimizing a loss function using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061a139-f603-40da-9cd3-94d288ef0587",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12dec4e2-11d2-42c3-a04c-4f6071fc53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 38.4003\n",
      "R-squared: -72.6859\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate a dataset\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.rand(80)\n",
    "\n",
    "# Define the number of trees (weak learners)\n",
    "n_trees = 100\n",
    "\n",
    "# Initialize the prediction with the mean of the target values\n",
    "prediction = np.mean(y)\n",
    "\n",
    "# Initialize a list to store individual tree predictions\n",
    "tree_predictions = []\n",
    "\n",
    "# Implement the gradient boosting algorithm\n",
    "for _ in range(n_trees):\n",
    "    # Calculate the residuals (errors)\n",
    "    residuals = y - prediction\n",
    "\n",
    "    # Fit a decision tree regressor to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X, residuals)\n",
    "\n",
    "    # Make predictions using the tree\n",
    "    tree_prediction = tree.predict(X)\n",
    "\n",
    "    # Update the prediction using a learning rate (e.g., 0.1)\n",
    "    learning_rate = 0.1\n",
    "    prediction += learning_rate * tree_prediction\n",
    "\n",
    "    # Append the current tree's prediction to the list\n",
    "    tree_predictions.append(tree_prediction)\n",
    "\n",
    "# Calculate the final prediction as the sum of individual tree predictions\n",
    "final_prediction = np.sum(tree_predictions, axis=0) + np.mean(y)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y, final_prediction)\n",
    "r2 = r2_score(y, final_prediction)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01ae90-95f2-4d2e-a075-2b0e7db3a5f0",
   "metadata": {},
   "source": [
    "## Q3.Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429f86c2-fb6e-4a84-9305-358fa4b63054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error (Test): 0.1227\n",
      "R-squared (Test): 0.7344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search using cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train a model with the best hyperparameters\n",
    "best_gb_model = GradientBoostingRegressor(**best_params)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (Test): {mse:.4f}\")\n",
    "print(f\"R-squared (Test): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20002488-a704-4d97-9225-09a566eca07b",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting? \n",
    "Ans:  A weak learner in Gradient Boosting is typically a decision tree with depth = 1, often referred to as a \"stump.\" Weak learners are simple models that perform slightly better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d67a3-c794-4571-b496-a46fa4213409",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans: The intuition behind the Gradient Boosting algorithm is to sequentially combine weak learners to create a strong learner. It focuses on minimizing the errors made by the previous learners, effectively reducing the overall prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498d71e-7ce8-4dad-83ad-9f67a0927cbb",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718f205-5f73-47c3-af0b-3ebfe195d541",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf7f3a60-a1db-4f99-8cac-5ef7f8e1f866",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
